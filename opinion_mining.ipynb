{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opinion mining from the dataset of Amazon product reviews\n",
    "\n",
    "The goal of this project is to mine user opinions about certain product features. The task will use a dataset of user reviews of several Amazon products. The goal is to analyze the reviews, extract product features and calculate sentiment per each feature. All the discussed source code is included in this document and described in the accompanying README.md. \n",
    "\n",
    "This document contains 4 cells:\n",
    "\n",
    "1. Dependancies and downloading corpora\n",
    "2. Model definition\n",
    "3. Run on one product and generate report\n",
    "4. Run on a set of products and generate results in markdown syntax\n",
    "\n",
    "The data is a collection of customer reviews, extracted from Amazon. Reviews for individual products are grouped in files and each file has been manually labelled with the list of product features, sentiment polarity and sentiment strength. Each file contains reviews for one specific product or domain. \n",
    "\n",
    "Symbols used in the annotated reviews (from Customer_review_data/Readme.txt): \n",
    "```text\n",
    "  [t]: the title of the review: Each [t] tag starts a review. \n",
    "       We did not use the title information in our papers.\n",
    "  xxxx[+|-n]: xxxx is a product feature. \n",
    "      [+n]: Positive opinion, n is the opinion strength: 3 strongest, \n",
    "            and 1 weakest. Note that the strength is quite subjective. \n",
    "            You may want ignore it, but only considering + and -\n",
    "      [-n]: Negative opinion\n",
    "  ##  : start of each sentence. Each line is a sentence. \n",
    "  [u] : feature not appeared in the sentence.\n",
    "  [p] : feature not appeared in the sentence. Pronoun resolution is needed.\n",
    "  [s] : suggestion or recommendation.\n",
    "  [cc]: comparison with a competing product from a different brand.\n",
    "  [cs]: comparison with a competing product from the same brand.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package product_reviews_1 to\n",
      "[nltk_data]     /Users/siemens/nltk_data...\n",
      "[nltk_data]   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data] Downloading package product_reviews_2 to\n",
      "[nltk_data]     /Users/siemens/nltk_data...\n",
      "[nltk_data]   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/siemens/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/siemens/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please run this cell to import deps and download the necessary corpora\n",
    "\n",
    "import nltk\n",
    "import numpy as np              # For TFIDF results handling\n",
    "import string\n",
    "import time                     # for timing execution\n",
    "import itertools                # For feature prunning\n",
    "import operator\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import product_reviews_1, product_reviews_2\n",
    "from textblob import TextBlob    # For noun phrase extraction and spell check\n",
    "from apyori import apriori       # For the Apriori algorithm\n",
    "from numpy import sign           # For extracting sentiment direction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('product_reviews_1')\n",
    "nltk.download('product_reviews_2')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Uncomment and run to see all available Amazon products in the corpora:\n",
    "# print(product_reviews_1.fileids())\n",
    "# print(product_reviews_2.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing internal state of the model\n",
    "\n",
    "The internal representation of both, the baseline and the upgraded model is built on top of the same architecture. They differ in the individual steps in the preprocessing, feature extraction or sentiment analysis pipeline and share the same internal representation. The main classes representing the  state of the model are based on the NLTK review reader module [2] and have been extended by my own classes to optimally store the internal state. The three core components are:\n",
    "\n",
    " ![class diagram](media/class_diagram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PSent:\n",
    "    \"\"\"PSent represents one sentence in a product review\"\"\"\n",
    "    \n",
    "    def __init__(self, s, f):\n",
    "        self.raw = s # Sentence in raw format\n",
    "        self.pp = [] # Sentence after preprocessing\n",
    "        self.ft = {} # Features / polarity based on model\n",
    "        self.op = [] # Opinion words\n",
    "        self.test = f # Test features labelled by human\n",
    "        self.eval = {} # Evaluation score\n",
    "        \n",
    "    def sentiment(self):\n",
    "        # Detecting sentiment polarity in the sentences                 \n",
    "        if self.ft:\n",
    "            # print(\"\\n\"+str(s))\n",
    "            p = TextBlob(str(self)).sentiment.polarity\n",
    "            if( p > 0 ):\n",
    "                p = 1                \n",
    "            elif( p < 0 ):\n",
    "                p = -1\n",
    "            # In base model there is no distintion\n",
    "            # between different features within one\n",
    "            # sentence:\n",
    "            for f in self.ft.keys():\n",
    "                self.ft[f] = p\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"PSent\"\n",
    "    \n",
    "    def __str__(self):\n",
    "        return ' '.join(self.raw)\n",
    "        \n",
    "class PRev:\n",
    "    \"\"\"PRev represents one review\"\"\"\n",
    "    \n",
    "    def __init__(self, r):\n",
    "        self.NltkReview = r # original review object\n",
    "        self.review = []    # preprocessed review   \n",
    "        self.eval = {}      # feature / sentiment evaluation score\n",
    "        \n",
    "    def preprocess(self, \n",
    "                         spelling = False,\n",
    "                         stemming = False,\n",
    "                         chunking = True,\n",
    "                         lemmatization = True):\n",
    "        \"\"\"Review preprocessing pipeline\n",
    "        \n",
    "        The following POS entities indicate the nouns:\n",
    "        * NN noun, singular ‘desk’\n",
    "        * NNS noun plural ‘desks’\n",
    "        * NNP proper noun, singular ‘Harrison’\n",
    "        * NNPS proper noun, plural ‘Americans’         \n",
    "        \"\"\"\n",
    "        \n",
    "        es = nltk.stem.SnowballStemmer('english')\n",
    "        \n",
    "        self.review = []\n",
    "        \n",
    "        for rl in self.NltkReview.review_lines:\n",
    "            \n",
    "            ps = PSent(rl.sent, \n",
    "                       rl.features) # Human labelled features\n",
    "            \n",
    "            a = ' '.join(rl.sent)\n",
    "            a = TextBlob(a)\n",
    "\n",
    "            # 2. spelling correction \n",
    "            if(spelling):                    \n",
    "                a = a.correct() \n",
    "            \n",
    "            # 3. get all nouns\n",
    "            nouns = []\n",
    "            for pos in a.tags:\n",
    "                if (pos[1][:2] == 'NN' \n",
    "                   and len(pos[0])>=3):\n",
    "                    nouns.append(pos[0])\n",
    "\n",
    "            # 4. Chunking noun phrases\n",
    "            if(chunking):\n",
    "                nouns = a.noun_phrases + nouns\n",
    "                # nouns = get_noun_phrases(' '.join(rl.sent)) + nouns\n",
    "\n",
    "            # 5. Lemmatication            \n",
    "            if(lemmatization):\n",
    "                nouns = [n.lemmatize() for n in nouns]\n",
    "            \n",
    "            \n",
    "            # 5. Stemming the words in the noun phrases\n",
    "            if(stemming):      \n",
    "                nouns = [es.stem(n) for n in nouns]\n",
    "            \n",
    "            ps.pp=nouns\n",
    "            self.review.append(ps)\n",
    "                \n",
    "            \n",
    "    def sents(self):\n",
    "        \"\"\"Return a list of processed sentences tokenized\"\"\"\n",
    "        \n",
    "        return [s.pp for s in self.review]\n",
    "    \n",
    "    def sents_raw(self):\n",
    "        \"\"\"Return a list of non-processed sentences tokenized\"\"\"\n",
    "        \n",
    "        return [s.raw for s in self.review]\n",
    "    \n",
    "    def sents_str(self):\n",
    "        \"\"\"Return processed sentences as one string\"\"\"\n",
    "        \n",
    "        rs = \"\"\n",
    "        for s in self.review:\n",
    "            rs +=' '.join(s.pp)\n",
    "        return rs\n",
    "            \n",
    "    def sentiment(self):\n",
    "        \"\"\"Detecting sentiment polarity in the sentences\n",
    "        containing product features\n",
    "        \"\"\"\n",
    "             \n",
    "        for s in self.review:               \n",
    "            s.sentiment()\n",
    "            \n",
    "    def features(self):\n",
    "        \"\"\"Return list of features mentioned in the review\"\"\"\n",
    "        \n",
    "        self.ft = []\n",
    "        for s in self.review:\n",
    "            self.ft.extend(s.ft.keys())\n",
    "        return list(set(self.ft))\n",
    "            \n",
    "            \n",
    "    def __str__(self):\n",
    "        \"\"\"Return original review as one string\"\"\"\n",
    "        \n",
    "        rstr = \"\"\n",
    "        for s in self.NltkReview.sents():\n",
    "            rstr += ' '.join(s)+'\\n'\n",
    "        return rstr\n",
    "\n",
    "    \n",
    "class PReviews:\n",
    "    \"\"\"PReviews represents a set of reviews for one product \n",
    "    e.g. set of user reviews for camera Canon G3\"\"\"\n",
    "    \n",
    "    def __init__(self, corpus, name):  \n",
    "        self.name = name        # Name of the product corpus\n",
    "        self.NltkCorpus = corpus.reviews(name)\n",
    "        self.eval = {}          # Evaluation score\n",
    "        self.report = {}        # Evaluation report\n",
    "        self.test_report = {}   # Target values (from annotations)\n",
    "        \n",
    "        self.revs = []\n",
    "        for r in self.NltkCorpus:\n",
    "            self.revs.append(PRev(r))\n",
    "            \n",
    "        \n",
    "    def preprocess(self, \n",
    "                         spelling = False,\n",
    "                         stemming = False,\n",
    "                         chunking = True,\n",
    "                         lemmatization = True):\n",
    "        \"\"\"Preprocess all reviews\"\"\"\n",
    "        \n",
    "        start = time.time()\n",
    "\n",
    "        for r in self.revs:\n",
    "            r.preprocess(spelling = spelling,\n",
    "                         stemming = stemming,\n",
    "                         chunking = chunking,\n",
    "                         lemmatization=lemmatization)\n",
    "            \n",
    "        end = time.time()\n",
    "\n",
    "        print(\"Preprocessed {} reviews in {:.2f} seconds (spelling correction={}, stemming={})\"\n",
    "              .format(len(self.revs), end-start, spelling, stemming))\n",
    "        \n",
    "    def get_tfidf_top_features(self, \n",
    "                               documents, \n",
    "                               tfidf_max_df=0.90, \n",
    "                               tfidf_min_df=0.05, \n",
    "                               tfidf_max_df_ngram_range=(1,3),\n",
    "                               n_top=30):\n",
    "        \"\"\"Calculate TFIDF matrix and return top N terms \n",
    "        with the highest TFIDF score\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        tfidf_vectorizer = TfidfVectorizer(max_df=tfidf_max_df, \n",
    "                                           min_df=tfidf_min_df, \n",
    "                                           stop_words='english', \n",
    "                                           ngram_range=tfidf_max_df_ngram_range)\n",
    "        tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "        importance = np.argsort(np.asarray(tfidf.sum(axis=0)).ravel())[::-1]\n",
    "        tfidf_feature_names = np.array(tfidf_vectorizer.get_feature_names())\n",
    "        \n",
    "        # Maybe distribute accross various n-gram lengths so that unigrams don't dominate?\n",
    "        #\n",
    "        # features_by_gram = defaultdict(list)\n",
    "        # for f, w in zip(tfidf_vectorizer.get_feature_names(), tfidf_vectorizer.idf_):\n",
    "        #    features_by_gram[len(f.split(' '))].append((f, w))\n",
    "\n",
    "        # print(tfidf_vectorizer.get_feature_names())\n",
    "\n",
    "        # for gram, features in features_by_gram.items():\n",
    "        #    top_features = sorted(features, key=lambda x: x[1], reverse=True)[:n_top]\n",
    "\n",
    "        #    top_features = [f[0] for f in top_features]\n",
    "        #    print('{}-gram top:{}'.format(gram, top_features))\n",
    "    \n",
    "        return tfidf_feature_names[importance[:n_top]]\n",
    "    \n",
    "    \n",
    "    def features(self,\n",
    "                 apr = True,\n",
    "                 prune = True,\n",
    "                 tfidf_max_df=0.90, \n",
    "                 tfidf_min_df=0.05, \n",
    "                 tfidf_max_df_ngram_range=(1,3),\n",
    "                 tfidf_top_n=10,\n",
    "                 min_support=0.005, \n",
    "                 min_confidence=0.2, \n",
    "                 min_lift=3, \n",
    "                 min_length=3):\n",
    "        \n",
    "        \"\"\"Mine product reviews for all product feature candidates.\"\"\"\n",
    "        \n",
    "        if not self.revs:\n",
    "            self.preprocess()\n",
    "        \n",
    "        sentence_dict = []       \n",
    "        for r in self.revs:\n",
    "            sentence_dict.extend(r.sents()) \n",
    "        \n",
    "        \n",
    "        #\n",
    "        # Extracting feature candidates based on the term frequency (TFIDF)\n",
    "        #\n",
    "        if len(self.revs) > 1:\n",
    "            # Normal review file with multiple reviews separated by title\n",
    "            top_n = list(self.get_tfidf_top_features([r.sents_str() for r in self.revs], \n",
    "                                                     tfidf_max_df=tfidf_max_df, \n",
    "                                                     tfidf_min_df=tfidf_min_df, \n",
    "                                                     tfidf_max_df_ngram_range=tfidf_max_df_ngram_range,\n",
    "                                                     n_top=tfidf_top_n))\n",
    "        else:\n",
    "            # Some review files contain one single title (one review with many lines)\n",
    "            top_n = list(self.get_tfidf_top_features([' '.join(s) for s in sentence_dict], n_top=30))\n",
    "            \n",
    "        \n",
    "                       \n",
    "        #\n",
    "        # Mining frequent items sets using the Apriori algorithm \n",
    "        # to find more potential features:        \n",
    "        #\n",
    "        ar=None\n",
    "        if apr:\n",
    "            self.association_rules = apriori(sentence_dict, \n",
    "                                        min_support=min_support, \n",
    "                                        min_confidence=min_confidence, \n",
    "                                        min_lift=min_lift, \n",
    "                                        min_length=min_length)\n",
    "\n",
    "            ar = list(self.association_rules)\n",
    "            for f in ar:\n",
    "                top_n.extend(list(f.items))\n",
    "        \n",
    "        # Drop duplicates:\n",
    "        top_n = list(set(top_n))\n",
    "        \n",
    "        #\n",
    "        #  Prunning the unwanted features:\n",
    "        #\n",
    "        if prune:\n",
    "            top_n = self.feature_prunning(top_n)\n",
    "        \n",
    "        # Tag each sentence in the corpus where one of the features is present:\n",
    "        self._tag_sentences_with_features(top_n, ar)\n",
    "        \n",
    "        return top_n\n",
    "    \n",
    "    def opinions(self):\n",
    "        \"\"\"Extracting opinion words (adjectives and adverbs)\n",
    "        JJ adjective, JJR adj comparative, JJS adj superlative\n",
    "        RB adverb, RBR comparative, RBS seperlative        \n",
    "        \"\"\"\n",
    "        \n",
    "        for r in self.revs:\n",
    "            for s in r.review:               \n",
    "                for w in TextBlob(str(s)).tags:\n",
    "                    if (w[1][:2] == 'JJ' or\n",
    "                       w[1][:2] == 'RB'):\n",
    "                            # print(w[0])\n",
    "                            s.op.append(w[0])\n",
    "    \n",
    "    def sentiment(self):\n",
    "        \"\"\"Detecting sentiment polarity in the sentences\n",
    "        containing product features        \n",
    "        \"\"\"\n",
    "        \n",
    "        for r in self.revs:\n",
    "            r.sentiment()\n",
    "                        \n",
    "    def gen_report(self):\n",
    "        \"\"\"Generate a feature report per product\"\"\"\n",
    "        \n",
    "        for r in self.revs:\n",
    "            for s in r.review:               \n",
    "                for feat, polarity in s.ft.items():\n",
    "                    self.report.setdefault(feat, (0,0))\n",
    "                    if polarity > 0:\n",
    "                        self.report[feat] = (self.report[feat][0]+1,\n",
    "                                             self.report[feat][1])\n",
    "                    elif polarity < 0:                \n",
    "                        self.report.setdefault(feat, (0,0))\n",
    "                        self.report[feat] = (self.report[feat][0],\n",
    "                                             self.report[feat][1]+1)\n",
    "            \n",
    "        return self.report\n",
    "    \n",
    "    def print_report(self, top_n=5):\n",
    "        \"\"\"Print the feature report per product\"\"\"\n",
    "        \n",
    "        if not self.report:\n",
    "            self.gen_report()\n",
    "            \n",
    "        sorted_dict = dict(sorted(self.report.items(), key=operator.itemgetter(1), reverse=True)[:top_n])\n",
    "            \n",
    "        print(\"\\nProduct: \", self.name)\n",
    "        for feat, score in sorted_dict.items():\n",
    "            print(\"\\t\\nFeature: \", feat)\n",
    "            print(\"\\t\\tPositive: \", sorted_dict[feat][0])\n",
    "            print(\"\\t\\tNegative: \", sorted_dict[feat][1])\n",
    "    \n",
    "    def extract_tagged_data(self):\n",
    "        self.test_report = {}\n",
    "        for r in self.NltkCorpus:\n",
    "            # print(\"\\n\")\n",
    "            for f in r.features():\n",
    "                feat = f[0] # name of feature\n",
    "                score_sign = f[1][0] #just the plus or minus sign\n",
    "                self.test_report.setdefault(feat, (0,0))\n",
    "                if score_sign == '+':\n",
    "                    self.test_report[feat] = (self.test_report[feat][0]+1,\n",
    "                                                 self.test_report[feat][1])\n",
    "                elif score_sign == '-':\n",
    "                    self.test_report[feat] = (self.test_report[feat][0],\n",
    "                                                 self.test_report[feat][1]+1)\n",
    "        return self.test_report\n",
    "    \n",
    "    def fscore(self, tp, fn, fp):\n",
    "        \"\"\"Calculate F-score\"\"\"\n",
    "        \n",
    "        recall = 0\n",
    "        prec = 0\n",
    "        fscore = 0\n",
    "        \n",
    "        if(tp+fn)>0:\n",
    "            recall = tp/(tp+fn) \n",
    "        \n",
    "        if(tp+fp)>0:\n",
    "            prec = tp/(tp+fp) \n",
    "            \n",
    "        if(recall+prec)>0:\n",
    "            fscore = 2*(recall*prec)/(recall+prec)\n",
    "        \n",
    "        return recall, prec, fscore\n",
    "    \n",
    "    def feat_evaluation(self, mute_output=False):\n",
    "        \"\"\"Evaluation of feature extraction success\n",
    "        Look at features that were picked up by mining and\n",
    "        those that were missed based on comparison with the\n",
    "        annotated data in NLTK corpus\n",
    "        \n",
    "        set mute_output to just return results without outputting to the console\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.test_report:\n",
    "            self.extract_tagged_data()\n",
    "            \n",
    "        if not self.report:\n",
    "            self.gen_report()\n",
    "            \n",
    "\n",
    "        f_tp = []\n",
    "        f_fp = []\n",
    "        f_fn = []\n",
    "        \n",
    "        for feat in self.report.keys():\n",
    "            if feat in self.test_report:\n",
    "                # cfmatch += 1\n",
    "                f_tp.append(feat)               \n",
    "            else:                \n",
    "                f_fp.append(feat)                \n",
    "                \n",
    "        for feat in self.test_report.keys():\n",
    "            if feat not in self.report:                \n",
    "                f_fn.append(feat)\n",
    "        \n",
    "        recall, prec, fscore = self.fscore(len(f_tp),\n",
    "                                           len(f_fn),\n",
    "                                           len(f_fp))\n",
    "        \n",
    "        results1 = [len(f_tp),len(f_fn),len(f_fp),recall,prec, fscore]\n",
    "    \n",
    "        if not mute_output:\n",
    "            print(\"Looking at all product features together:\")\n",
    "            print(\"|\\tTP\\t|\\tFN\\t|\\tFP\\t|\\tRecall\\t|\\tPrecision\\t|\\tF-score\\t|\")\n",
    "            print(\"|---|---|---|---|---|---|\")\n",
    "            print(\"|\\t{0}\\t|\\t{1}\\t|\\t{2}\\t|\\t{3:.2f}\\t|\\t{4:.2f}\\t|\\t{5:.2f}\\t|\".format(len(f_tp), \n",
    "                                                                                         len(f_fn), \n",
    "                                                                                         len(f_fp),\n",
    "                                                                                        recall,\n",
    "                                                                                        prec,\n",
    "                                                                                        fscore))\n",
    "        \n",
    "            print(\"\\nTP\")\n",
    "            print('%s' % ', '.join(map(str, f_tp)))\n",
    "\n",
    "            print(\"\\nFN\")\n",
    "            print('%s' % ', '.join(map(str, f_fn)))\n",
    "\n",
    "            print(\"\\nFP\")\n",
    "            print('%s' % ', '.join(map(str, f_fp)))\n",
    "\n",
    "        \n",
    "        # Calculate recall / precision and F1 score per sentence, review and product:\n",
    "        self.eval[\"tp\"] = 0 # true positive (labelled feature found in mined features)\n",
    "        self.eval[\"fn\"] = 0 # false negative (labelled feature not found in mined features)\n",
    "        self.eval[\"fp\"] = 0 # false positive, mined feature that is not present in labelled\n",
    "        self.eval[\"recall\"] = 0 \n",
    "        self.eval[\"prec\"] = 0 \n",
    "        self.eval[\"fscore\"] = 0 \n",
    "        for r in self.revs:\n",
    "            r.eval[\"tp\"] = 0 # true positive (labelled feature found in mined features)\n",
    "            r.eval[\"fn\"] = 0 # false negative (labelled feature not found in mined features)\n",
    "            r.eval[\"fp\"] = 0 # false positive, mined feature that is not present in labelled\n",
    "            r.eval[\"recall\"] = 0 \n",
    "            r.eval[\"prec\"] = 0 \n",
    "            r.eval[\"fscore\"] = 0 \n",
    "            for s in r.review: # iterate through each sentence\n",
    "                s.eval[\"tp\"] = 0 # true positive (labelled feature found in mined features)\n",
    "                s.eval[\"fn\"] = 0    # false negative (labelled feature not found in mined features)\n",
    "                s.eval[\"fp\"] = 0    # false positive, mined feature that is not present in labelled\n",
    "                s.eval[\"recall\"] = 0 \n",
    "                s.eval[\"prec\"] = 0 \n",
    "                s.eval[\"fscore\"] = 0 \n",
    "                for lf in s.test: # iterate through labelled features\n",
    "                    if lf[0] in s.ft.keys(): # comparing to mined features\n",
    "                        s.eval[\"tp\"] += 1\n",
    "                    else:                        \n",
    "                        s.eval[\"fn\"] += 1\n",
    "                for mf in s.ft.keys(): # iterate through labelled features\n",
    "                    found = False\n",
    "                    for lf in s.test:                        \n",
    "                        if mf == lf[0]:\n",
    "                            found = True\n",
    "                            break\n",
    "                    if not found:\n",
    "                        s.eval[\"fp\"] += 1                        \n",
    "                        \n",
    "                r.eval[\"tp\"] += s.eval[\"tp\"] # true positive (labelled feature found in mined features)\n",
    "                r.eval[\"fn\"] += s.eval[\"fn\"] # false negative (labelled feature not found in mined features)\n",
    "                r.eval[\"fp\"] += s.eval[\"fp\"] # false positive, mined feature that is not present in labelled\n",
    "                \n",
    "                # Recall / precision / F1 score per sentence:\n",
    "                s.eval[\"recall\"], s.eval[\"prec\"], s.eval[\"fscore\"] = self.fscore(s.eval[\"tp\"],\n",
    "                                                                               s.eval[\"fn\"],\n",
    "                                                                               s.eval[\"fp\"])\n",
    "                \n",
    "            self.eval[\"tp\"] += r.eval[\"tp\"] # true positive (labelled feature found in mined features)\n",
    "            self.eval[\"fn\"] += r.eval[\"fn\"] # false negative (labelled feature not found in mined features)\n",
    "            self.eval[\"fp\"] += r.eval[\"fp\"] # false positive, mined feature that is not present in labelled\n",
    "                \n",
    "            # Recall / precision / F1 score per review:\n",
    "            r.eval[\"recall\"], r.eval[\"prec\"], r.eval[\"fscore\"] = self.fscore(r.eval[\"tp\"],\n",
    "                                                                           r.eval[\"fn\"],\n",
    "                                                                           r.eval[\"fp\"])\n",
    "            \n",
    "        # Recall / precision / F1 score per product:\n",
    "        self.eval[\"recall\"], self.eval[\"prec\"], self.eval[\"fscore\"] = self.fscore(self.eval[\"tp\"],\n",
    "                                                                                   self.eval[\"fn\"],\n",
    "                                                                                   self.eval[\"fp\"])\n",
    "        \n",
    "        results2 = [self.eval[\"tp\"],\n",
    "                    self.eval[\"fn\"],\n",
    "                    self.eval[\"fp\"],\n",
    "                    self.eval[\"recall\"],\n",
    "                    self.eval[\"prec\"],\n",
    "                    self.eval[\"fscore\"]]\n",
    "        if not mute_output:\n",
    "            print(\"\\n\\nLooking at product features per individual sentence:\")\n",
    "            print(\"|\\tTP\\t|\\tFN\\t|\\tFP\\t|\\tRecall\\t|\\tPrec\\t|\\tF score\\t|\")\n",
    "            print(\"|\\t{0}\\t|\\t{1}\\t|\\t{2}\\t|\\t{3:.2f}\\t|\\t{4:.2f}\\t|\\t{5:.2f}\\t|\"\n",
    "                  .format(self.eval[\"tp\"],\n",
    "                        self.eval[\"fn\"],\n",
    "                        self.eval[\"fp\"],\n",
    "                        self.eval[\"recall\"],\n",
    "                        self.eval[\"prec\"],\n",
    "                        self.eval[\"fscore\"]))\n",
    "\n",
    "        return results1, results2\n",
    "        \n",
    "    def sent_evaluation(self, mute_output=False):   \n",
    "        \"\"\"Evaluation of sentiment analysis success\n",
    "        \n",
    "        set mute_output to just return results without outputting to the console\n",
    "        \"\"\"\n",
    "        \n",
    "        if not self.test_report:\n",
    "            self.extract_tagged_data()\n",
    "            \n",
    "        if not self.report:\n",
    "            self.gen_report()\n",
    "            \n",
    "        # First collect features that were correctly mined\n",
    "        f_tp = []        \n",
    "        for feat in self.report.keys():\n",
    "            if feat in self.test_report:\n",
    "                f_tp.append(feat)\n",
    "\n",
    "        # Calculate recall / precision and F1 score per sentence, review and product\n",
    "        # on those sentences that contain features that were correctly mined:\n",
    "        self.eval[\"stp\"] = 0 # true positive (positive feature labelled positive)\n",
    "        self.eval[\"sfn\"] = 0 # false negative (positive feature not labelled positive)\n",
    "        self.eval[\"sfp\"] = 0 # false positive (negative feature labelled positive)\n",
    "        self.eval[\"srecall\"] = 0 \n",
    "        self.eval[\"sprec\"] = 0 \n",
    "        self.eval[\"sfscore\"] = 0 \n",
    "        for r in self.revs:\n",
    "            r.eval[\"stp\"] = 0 # true positive (positive feature labelled positive)\n",
    "            r.eval[\"sfn\"] = 0 # false negative (positive feature not labelled positive)\n",
    "            r.eval[\"sfp\"] = 0 # false positive (negative feature labelled positive)\n",
    "            r.eval[\"srecall\"] = 0 \n",
    "            r.eval[\"sprec\"] = 0 \n",
    "            r.eval[\"sfscore\"] = 0 \n",
    "            for s in r.review: # iterate through each sentence\n",
    "                s.eval[\"stp\"] = 0 # true positive (positive feature labelled positive)\n",
    "                s.eval[\"sfn\"] = 0    # false negative (positive feature not labelled positive)\n",
    "                s.eval[\"sfp\"] = 0    # false positive (negative feature labelled positive)\n",
    "                s.eval[\"srecall\"] = 0 \n",
    "                s.eval[\"sprec\"] = 0 \n",
    "                s.eval[\"sfscore\"] = 0 \n",
    "                for lf in s.test: # iterate through labelled features\n",
    "                    if lf[0] in s.ft: # comparing to mined features\n",
    "                        if sign(int(lf[1])) == s.ft[lf[0]]:\n",
    "                            s.eval[\"stp\"] += 1\n",
    "                            # print(\"TP {}:{}=={}:{}\".format(lf[0],sign(int(lf[1])),lf[0], s.ft[lf[0]]))\n",
    "                        elif sign(int(lf[1])) == 1.0:\n",
    "                            s.eval[\"sfn\"] += 1\n",
    "                            # print(\"Falsely negative:\")\n",
    "                            # print(s)\n",
    "                        else:\n",
    "                            s.eval[\"sfp\"] += 1\n",
    "                            # print(\"Falsely positive:\")\n",
    "                            # print(s)\n",
    "                        \n",
    "                r.eval[\"stp\"] += s.eval[\"stp\"] # true positive (positive feature labelled positive)\n",
    "                r.eval[\"sfn\"] += s.eval[\"sfn\"] # false negative (positive feature not labelled positive)\n",
    "                r.eval[\"sfp\"] += s.eval[\"sfp\"] # false positive (negative feature labelled positive)\n",
    "                \n",
    "                # Recall / precision / F1 score per sentence:\n",
    "                s.eval[\"srecall\"], s.eval[\"sprec\"], s.eval[\"sfscore\"] = self.fscore(s.eval[\"stp\"],\n",
    "                                                                               s.eval[\"sfn\"],\n",
    "                                                                               s.eval[\"sfp\"])\n",
    "                \n",
    "            self.eval[\"stp\"] += r.eval[\"stp\"] # true positive (positive feature labelled positive)\n",
    "            self.eval[\"sfn\"] += r.eval[\"sfn\"] # false negative (positive feature not labelled positive)\n",
    "            self.eval[\"sfp\"] += r.eval[\"sfp\"] # false positive (negative feature labelled positive)\n",
    "                \n",
    "            # Recall / precision / F1 score per review:\n",
    "            r.eval[\"srecall\"], r.eval[\"sprec\"], r.eval[\"sfscore\"] = self.fscore(r.eval[\"stp\"],\n",
    "                                                                           r.eval[\"sfn\"],\n",
    "                                                                           r.eval[\"sfp\"])\n",
    "            \n",
    "        # Recall / precision / F1 score per product:\n",
    "        self.eval[\"srecall\"], self.eval[\"sprec\"], self.eval[\"sfscore\"] = self.fscore(self.eval[\"stp\"],\n",
    "                                                                                   self.eval[\"sfn\"],\n",
    "                                                                                   self.eval[\"sfp\"])\n",
    "        results = [self.eval[\"stp\"],\n",
    "                    self.eval[\"sfn\"],\n",
    "                    self.eval[\"sfp\"],\n",
    "                    self.eval[\"srecall\"],\n",
    "                    self.eval[\"sprec\"],\n",
    "                    self.eval[\"sfscore\"]]\n",
    "        if not mute_output:\n",
    "            print(\"\\n\\nLooking at sentiment evaluation per individual sentence:\")\n",
    "            print(\"|\\tTP\\t|\\tFN\\t|\\tFP\\t|\\tRecall\\t|\\tPrec\\t|\\tF score\\t|\")\n",
    "            print(\"|\\t{0}\\t|\\t{1}\\t|\\t{2}\\t|\\t{3:.2f}\\t|\\t{4:.2f}\\t|\\t{5:.2f}\\t|\"\n",
    "                  .format(self.eval[\"stp\"],\n",
    "                        self.eval[\"sfn\"],\n",
    "                        self.eval[\"sfp\"],\n",
    "                        self.eval[\"srecall\"],\n",
    "                        self.eval[\"sprec\"],\n",
    "                        self.eval[\"sfscore\"]))\n",
    "        return results\n",
    "    \n",
    "    def d(w1, w2, words):\n",
    "        \"\"\"Calculate distance between two words in a sentence\n",
    "        \n",
    "        Consider that the words might appear many times in a single sentence.\n",
    "        In such case the minimum is calculated.\n",
    "        \"\"\"\n",
    "        \n",
    "        if w1 in words and w2 in words:\n",
    "            w1_indexes = [index for index, value in enumerate(words) if value == w1]    \n",
    "            w2_indexes = [index for index, value in enumerate(words) if value == w2]    \n",
    "            distances = [abs(item[0] - item[1]) for item in itertools.product(w1_indexes, w2_indexes)]\n",
    "            return {'min': min(distances), 'avg': sum(distances)/float(len(distances))}\n",
    "    \n",
    "    def compactness_prunning(self, multiple_word_features):\n",
    "        \"\"\"Prune features containing multiple words \n",
    "        based on the distance between those words in a sentence\n",
    "        \"\"\"\n",
    "        \n",
    "        exclude = []\n",
    "        for (mw, parts) in multiple_word_features:\n",
    "            compact_count = 0\n",
    "            if len(parts) == 2:\n",
    "                for r in self.revs:\n",
    "                    for s in r.review:\n",
    "                        if mw in s.ft:\n",
    "                            distance = d(parts[0],parts[1],s.raw)\n",
    "                            if distance and distance[\"min\"] <= 3:\n",
    "                                # print(\"{} is compact in {}\".format(mw,s.raw))\n",
    "                                compact_count+=1\n",
    "                    if compact_count >= 2:\n",
    "                        break\n",
    "            if compact_count < 2:\n",
    "                exclude.append(mw)\n",
    "        return exclude            \n",
    "    \n",
    "    def feature_prunning(self, features):\n",
    "        \"\"\"Prune features based on compactness prunning and\n",
    "        redundancy prunning\n",
    "        \"\"\"\n",
    "        \n",
    "        hierarchy = {}\n",
    "        multiword = []\n",
    "        exclude = []\n",
    "        \n",
    "        count_start = len(features)\n",
    "\n",
    "        for f in features:\n",
    "            parts = f.split(' ')\n",
    "            if len(parts) == 1:\n",
    "                hierarchy[f]=[]\n",
    "            else:\n",
    "                # check if words in the multi-word feature repeat\n",
    "                if not len(set(parts)) == len(list(parts)):\n",
    "                    #print(\"Exclude (repetition): \", f)\n",
    "                    exclude.append(f)\n",
    "                else:\n",
    "                    multiword.append((f,parts))\n",
    "                    \n",
    "        exclude.extend(self.compactness_prunning(multiword))\n",
    "\n",
    "        for mw, parts in multiword:\n",
    "            for p in parts:\n",
    "                if p in hierarchy.keys():\n",
    "                    # multiword feature is a narrower category of feature p\n",
    "                    hierarchy[p].append(mw)\n",
    "\n",
    "        document_presence = {}\n",
    "        for w, more_specific in hierarchy.items():\n",
    "            for mw in more_specific:\n",
    "                both = 0\n",
    "                general = 0\n",
    "                for r in self.revs:\n",
    "                    if mw in r.sents_str():\n",
    "                        both += 1\n",
    "                    elif w in r.sents_str():\n",
    "                        general += 1\n",
    "                # print(\"{} vs {}, both {} general {} ratio {}\".format(w,mw,both,general,(general+both)/both))\n",
    "                if both > 0 and general/both == 1:                    \n",
    "                    exclude.append(w)\n",
    "        \n",
    "        result = [f for f in features if f not in exclude]\n",
    "        count_end = len(result)\n",
    "        \n",
    "        print(\"Pruned {} out of {} features\".format(count_start-count_end,count_start))\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "    def _tag_sentences_with_features(self, features, ar=None):\n",
    "        for r in self.revs:\n",
    "            for s in r.review:\n",
    "                found = False\n",
    "                for word in features:\n",
    "                    if word in s.pp:                        \n",
    "                        s.ft[word]=0\n",
    "                        \n",
    "                if ar:\n",
    "                    for f in ar:\n",
    "                        feat = list(f.items)  \n",
    "\n",
    "                        found = False\n",
    "                        for word in feat:\n",
    "                            found = False\n",
    "                            for np in s.pp:\n",
    "                                if word == np or word in np:\n",
    "                                    found = True\n",
    "                                    break\n",
    "                            if not found:\n",
    "                                break                \n",
    "                        if(found):                        \n",
    "                            featstr = feat[1]+' '+feat[0]\n",
    "                            s.ft[featstr]=0\n",
    "        \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process one product and generate the feature report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 45 reviews in 4.33 seconds (spelling correction=False, stemming=False)\n",
      "Pruned 30 out of 134 features\n",
      "Looking at all product features together:\n",
      "|\tTP\t|\tFN\t|\tFP\t|\tRecall\t|\tPrecision\t|\tF-score\t|\n",
      "|---|---|---|---|---|---|\n",
      "|\t32\t|\t73\t|\t177\t|\t0.30\t|\t0.15\t|\t0.20\t|\n",
      "\n",
      "TP\n",
      "canon, camera, picture, quality, picture quality, flash, feature, use, option, software, control, lens, image, dial, viewfinder, photo, lcd, design, focus, zoom, battery, shoot, lens cap, price, color, shot, product, lag, lag time, compactflash, performance, strap\n",
      "\n",
      "FN\n",
      "canon powershot g3, speed, function, auto setting, canon g3, photo quality, darn diopter adjustment dial, exposure control, metering option, spot metering, 4mp, size, weight, optical zoom, digital zoom, menu, button, lense, auto mode, canera, print, manual mode, feel, four megapixel, night mode, lens cover, zooming lever, white balance, grain, flash photo, noise, g3, depth, external flash hot shoe, raw image, battery life, manual function, service, automode, raw format, shape, light auto correction, white offset, low light focus, unresponsiveness, delay, 4mp camera, body, casing, look, finish, tiff format, import, manual, stitch picture, display, memory card, made, lever, learning, image quality, macro, 4mp resolution, distortion, learning curve, remote, hot shoe flash, battery charging system, highlight, off button, download, optic, digital camera\n",
      "\n",
      "FP\n",
      "week, picture week, way, work, picture quality picture, quality picture, picture quality quality, setting, card, choice, line, flash external flash, ease, use ease, digital camera camera, month, megapixel, camera powershot g3, quality image, thing, finder view, view lens, lens viewfinder, resolution, review, sample, online, online camera, image resolution, online review, review camera, mode, canon g3 canon, quality photo, shutter, camera digital cameras, speed shutter, shutter speed speed, camera shutter, flaw, time, finder lcd, lens lcd, view lcd, viewfinder lcd, flaw design, camera design, photography, digital photography photography, point photography, day, screen, adjustment, exposure, slr, auto, photo auto, setting exposure, range, lcd lcd screen, lcd screen, lcd screen screen, market, camera market, film, result, film exposure, picture screen, point, point shoot, zoom picture, problem, picture digital pictures, picture problem, view, cap, cap lens cap, lens lens cap, camera lens cap, camera lcd, quality megapixel, difference, zoom lens, powershot, picture great pictures, pic, auto mode auto, mode auto, auto mode mode, camera photo, store, olympus, life, type, battery battery life, life battery, battery type, life battery life, camera canon, battery camera, battery life life, finder, amazon, amazon time, great camera camera, shot photo, priority, mode shutter, mode speed, priority shutter, something, lot, camera shoot, picture color, quality color, experience, experience camera, light, printer, digital, light quality, quality flash, digicams, slr lens, lock, lock lag, lock time, point digicams, focus lag, lock focus, anything, anything time, something time, metz, shoe, flash metz, flash shoe, nikon canon, flash metz flash, metz metz flash, raw image image, image software, adobe, camera adobe, image color, quality resolution, consumer, camera consumer, feature option, effect, expectation, processing, image image quality, quality image quality, nikon, way picture, pro, moment, film moment, camera moment, barrel, unit, camera strap, corner, corner lcd, corner lens, corner viewfinder, flash compact flash, finder view finder, view view finder, coolpix, nikon coolpix, nikon coolpix coolpix, nikon nikon coolpix, coolpix nikon coolpix, unit flash, difference photo\n",
      "\n",
      "\n",
      "Looking at product features per individual sentence:\n",
      "|\tTP\t|\tFN\t|\tFP\t|\tRecall\t|\tPrec\t|\tF score\t|\n",
      "|\t140\t|\t145\t|\t1647\t|\t0.49\t|\t0.08\t|\t0.14\t|\n",
      "\n",
      "Product:  Canon_G3.txt\n",
      "\t\n",
      "Feature:  camera\n",
      "\t\tPositive:  128\n",
      "\t\tNegative:  25\n",
      "\t\n",
      "Feature:  picture\n",
      "\t\tPositive:  41\n",
      "\t\tNegative:  5\n",
      "\t\n",
      "Feature:  quality\n",
      "\t\tPositive:  30\n",
      "\t\tNegative:  2\n",
      "\t\n",
      "Feature:  canon\n",
      "\t\tPositive:  25\n",
      "\t\tNegative:  8\n",
      "\t\n",
      "Feature:  digital camera camera\n",
      "\t\tPositive:  25\n",
      "\t\tNegative:  2\n"
     ]
    }
   ],
   "source": [
    "c = PReviews(product_reviews_1, 'Canon_G3.txt')\n",
    "c.preprocess(chunking=True, lemmatization=True)\n",
    "c.features(apr = True,\n",
    "           prune=True,\n",
    "                 tfidf_max_df=0.80, \n",
    "                 tfidf_min_df=0.03, \n",
    "                 tfidf_max_df_ngram_range=(1,3),\n",
    "                 tfidf_top_n=100,\n",
    "                 min_support=0.004, \n",
    "                 min_confidence=0.2, \n",
    "                 min_lift=3, \n",
    "                 min_length=3)\n",
    "c.opinions()\n",
    "c.sentiment()\n",
    "c.feat_evaluation(mute_output=False)\n",
    "c.sent_evaluation(mute_output=True)\n",
    "c.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating results on multiple products / markdown generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed 99 reviews in 0.76 seconds (spelling correction=False, stemming=False)\n",
      "Pruned 29 out of 134 features\n",
      "Preprocessed 45 reviews in 0.83 seconds (spelling correction=False, stemming=False)\n",
      "Pruned 78 out of 240 features\n",
      "Preprocessed 95 reviews in 1.85 seconds (spelling correction=False, stemming=False)\n",
      "Pruned 14 out of 113 features\n",
      "Preprocessed 34 reviews in 0.40 seconds (spelling correction=False, stemming=False)\n",
      "Pruned 44 out of 159 features\n",
      "Preprocessed 40 reviews in 0.79 seconds (spelling correction=False, stemming=False)\n",
      "Pruned 45 out of 181 features\n",
      "Looking at features per product:\n",
      "|\tName\t|\tTP\t|\tFN\t|\tFP\t|\tRecall\t|\tPrecision\t|\tF-score\t|\n",
      "|:---|:---:|:---:|:---:|:---:|:---:|:---:|\n",
      "|\tApex_AD2600_Progressive_scan_DVD player.txt\t|\t39\t|\t76\t|\t171\t|\t0.34\t|\t0.19\t|\t0.24\t|\n",
      "|\tCanon_G3.txt\t|\t41\t|\t64\t|\t554\t|\t0.39\t|\t0.07\t|\t0.12\t|\n",
      "|\tCreative_Labs_Nomad_Jukebox_Zen_Xtra_40GB.txt\t|\t47\t|\t141\t|\t133\t|\t0.25\t|\t0.26\t|\t0.26\t|\n",
      "|\tNikon_coolpix_4300.txt\t|\t22\t|\t53\t|\t266\t|\t0.29\t|\t0.08\t|\t0.12\t|\n",
      "|\tNokia_6610.txt\t|\t44\t|\t67\t|\t328\t|\t0.40\t|\t0.12\t|\t0.18\t|\n",
      "|\tAverage values\t|\t**38.60**\t|\t**80.20**\t|\t**290.40**\t|\t**0.33**\t|\t**0.14**\t|\t**0.18**\t|\n",
      "Looking at features per sentence:\n",
      "|\tName\t|\tTP\t|\tFN\t|\tFP\t|\tRecall\t|\tPrecision\t|\tF-score\t|\n",
      "|:---|:---:|:---:|:---:|:---:|:---:|:---:|\n",
      "|\tApex_AD2600_Progressive_scan_DVD player.txt\t|\t190\t|\t240\t|\t2093\t|\t0.44\t|\t0.08\t|\t0.14\t|\n",
      "|\tCanon_G3.txt\t|\t150\t|\t135\t|\t2596\t|\t0.53\t|\t0.05\t|\t0.10\t|\n",
      "|\tCreative_Labs_Nomad_Jukebox_Zen_Xtra_40GB.txt\t|\t397\t|\t448\t|\t4107\t|\t0.47\t|\t0.09\t|\t0.15\t|\n",
      "|\tNikon_coolpix_4300.txt\t|\t95\t|\t108\t|\t1222\t|\t0.47\t|\t0.07\t|\t0.12\t|\n",
      "|\tNokia_6610.txt\t|\t168\t|\t170\t|\t1586\t|\t0.50\t|\t0.10\t|\t0.16\t|\n",
      "|\tAverage values\t|\t**200.00**\t|\t**220.20**\t|\t**2320.80**\t|\t**0.48**\t|\t**0.08**\t|\t**0.13**\t|\n",
      "Looking at sentiment analysis:\n",
      "|\tName\t|\tTP\t|\tFN\t|\tFP\t|\tRecall\t|\tPrecision\t|\tF-score\t|\n",
      "|:---|:---:|:---:|:---:|:---:|:---:|:---:|\n",
      "|\tApex_AD2600_Progressive_scan_DVD player.txt\t|\t103\t|\t15\t|\t72\t|\t0.87\t|\t0.59\t|\t0.70\t|\n",
      "|\tCanon_G3.txt\t|\t118\t|\t15\t|\t17\t|\t0.89\t|\t0.87\t|\t0.88\t|\n",
      "|\tCreative_Labs_Nomad_Jukebox_Zen_Xtra_40GB.txt\t|\t260\t|\t43\t|\t94\t|\t0.86\t|\t0.73\t|\t0.79\t|\n",
      "|\tNikon_coolpix_4300.txt\t|\t73\t|\t15\t|\t7\t|\t0.83\t|\t0.91\t|\t0.87\t|\n",
      "|\tNokia_6610.txt\t|\t121\t|\t24\t|\t23\t|\t0.83\t|\t0.84\t|\t0.84\t|\n",
      "|\tAverage values\t|\t**135.00**\t|\t**22.40**\t|\t**42.60**\t|\t**0.86**\t|\t**0.79**\t|\t**0.82**\t|\n"
     ]
    }
   ],
   "source": [
    "# This is a bit messy and is intended to output evaluation results of the opinion miner \n",
    "# for all the products in both corpora formatted as markdown.\n",
    "\n",
    "results1 = {}\n",
    "results2 = {}\n",
    "results3 = {}\n",
    "sum1=[0.0,0.0,0.0,0.0,0.0,0.0]\n",
    "sum2=[0.0,0.0,0.0,0.0,0.0,0.0]\n",
    "sum3=[0.0,0.0,0.0,0.0,0.0,0.0]\n",
    "\n",
    "count = 0\n",
    "for corpus in [product_reviews_1, product_reviews_2]:\n",
    "    for product in corpus.fileids():\n",
    "        if count < 5:            \n",
    "            if product not in ['README.txt','ipod.txt', 'norton.txt']:            \n",
    "                c = PReviews(corpus, product)\n",
    "                c.preprocess(chunking=True, lemmatization=True, spelling = False, stemming = False)\n",
    "                c.features(apr = True,\n",
    "               prune=True,\n",
    "                     tfidf_max_df=0.80, \n",
    "                     tfidf_min_df=0.03, \n",
    "                     tfidf_max_df_ngram_range=(1,3),\n",
    "                     tfidf_top_n=100,\n",
    "                     min_support=0.003, \n",
    "                     min_confidence=0.2, \n",
    "                     min_lift=3, \n",
    "                     min_length=3)\n",
    "                c.opinions()\n",
    "                c.sentiment()\n",
    "                r1,r2 = c.feat_evaluation(mute_output=True)\n",
    "                r3 = c.sent_evaluation(mute_output=True)\n",
    "                results1[product]=r1\n",
    "                results2[product]=r2\n",
    "                results3[product]=r3\n",
    "                sum1 = [x + y for x, y in zip(sum1, r1)]            \n",
    "                sum2 = [x + y for x, y in zip(sum2, r2)]\n",
    "                sum3 = [x + y for x, y in zip(sum3, r3)]\n",
    "                count+=1\n",
    "            \n",
    "sum1=[x / count for x in sum1]\n",
    "sum2=[x / count for x in sum2]\n",
    "sum3=[x / count for x in sum3]\n",
    "\n",
    "print(\"Looking at features per product:\")\n",
    "print(\"|\\tName\\t|\\tTP\\t|\\tFN\\t|\\tFP\\t|\\tRecall\\t|\\tPrecision\\t|\\tF-score\\t|\")\n",
    "print(\"|:---|:---:|:---:|:---:|:---:|:---:|:---:|\")\n",
    "for product, r in results1.items():\n",
    "    print(\"|\\t{6}\\t|\\t{0}\\t|\\t{1}\\t|\\t{2}\\t|\\t{3:.2f}\\t|\\t{4:.2f}\\t|\\t{5:.2f}\\t|\".format(r[0], \n",
    "                                                                                 r[1], \n",
    "                                                                                 r[2],\n",
    "                                                                                r[3],\n",
    "                                                                                r[4],\n",
    "                                                                                r[5],\n",
    "                                                                                        product))\n",
    "print(\"|\\t{6}\\t|\\t**{0:.2f}**\\t|\\t**{1:.2f}**\\t|\\t**{2:.2f}**\\t|\\t**{3:.2f}**\\t|\\t**{4:.2f}**\\t|\\t**{5:.2f}**\\t|\".format(sum1[0], \n",
    "                                                                             sum1[1], \n",
    "                                                                             sum1[2],\n",
    "                                                                            sum1[3],\n",
    "                                                                            sum1[4],\n",
    "                                                                            sum1[5],\n",
    "                                                                                    \"Average values\"))\n",
    "    \n",
    "print(\"Looking at features per sentence:\")\n",
    "print(\"|\\tName\\t|\\tTP\\t|\\tFN\\t|\\tFP\\t|\\tRecall\\t|\\tPrecision\\t|\\tF-score\\t|\")\n",
    "print(\"|:---|:---:|:---:|:---:|:---:|:---:|:---:|\")\n",
    "for product, r in results2.items():\n",
    "    print(\"|\\t{6}\\t|\\t{0}\\t|\\t{1}\\t|\\t{2}\\t|\\t{3:.2f}\\t|\\t{4:.2f}\\t|\\t{5:.2f}\\t|\".format(r[0], \n",
    "                                                                                 r[1], \n",
    "                                                                                 r[2],\n",
    "                                                                                r[3],\n",
    "                                                                                r[4],\n",
    "                                                                                r[5],\n",
    "                                                                                        product))\n",
    "print(\"|\\t{6}\\t|\\t**{0:.2f}**\\t|\\t**{1:.2f}**\\t|\\t**{2:.2f}**\\t|\\t**{3:.2f}**\\t|\\t**{4:.2f}**\\t|\\t**{5:.2f}**\\t|\".format(sum2[0], \n",
    "                                                                             sum2[1], \n",
    "                                                                             sum2[2],\n",
    "                                                                            sum2[3],\n",
    "                                                                            sum2[4],\n",
    "                                                                            sum2[5],\n",
    "                                                                                    \"Average values\"))\n",
    "    \n",
    "print(\"Looking at sentiment analysis:\")\n",
    "print(\"|\\tName\\t|\\tTP\\t|\\tFN\\t|\\tFP\\t|\\tRecall\\t|\\tPrecision\\t|\\tF-score\\t|\")\n",
    "print(\"|:---|:---:|:---:|:---:|:---:|:---:|:---:|\")\n",
    "for product, r in results3.items():\n",
    "    print(\"|\\t{6}\\t|\\t{0}\\t|\\t{1}\\t|\\t{2}\\t|\\t{3:.2f}\\t|\\t{4:.2f}\\t|\\t{5:.2f}\\t|\".format(r[0], \n",
    "                                                                                 r[1], \n",
    "                                                                                 r[2],\n",
    "                                                                                r[3],\n",
    "                                                                                r[4],\n",
    "                                                                                r[5],\n",
    "                                                                                        product))\n",
    "print(\"|\\t{6}\\t|\\t**{0:.2f}**\\t|\\t**{1:.2f}**\\t|\\t**{2:.2f}**\\t|\\t**{3:.2f}**\\t|\\t**{4:.2f}**\\t|\\t**{5:.2f}**\\t|\".format(sum3[0], \n",
    "                                                                             sum3[1], \n",
    "                                                                             sum3[2],\n",
    "                                                                            sum3[3],\n",
    "                                                                            sum3[4],\n",
    "                                                                            sum3[5],\n",
    "                                                                                    \"Average values\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manually parsing the corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tagged_reviews(path):\n",
    "    with open(path, 'r') as f:\n",
    "        reviews = []\n",
    "        \n",
    "        title = \"\"\n",
    "        text = []\n",
    "        for line in f.readlines():\n",
    "            if line.startswith(\"*\"): \n",
    "                # skip comment                \n",
    "                continue\n",
    "            elif line.startswith(\"[t]\"):                 \n",
    "                # title of new review\n",
    "                if text: # but title can be empty                    \n",
    "                    reviews.append(text)                \n",
    "                text = [] # reset last review\n",
    "                features = \"\" # reset last feature\n",
    "                title = line[3:]\n",
    "                # print(\"Title:\", title)                \n",
    "            elif line.startswith(\"##\"): # sentence\n",
    "                text.append(line[2:])\n",
    "            elif not line.startswith(\"##\") and \"##\" in line: #feature\n",
    "                s = line.split(\"##\")\n",
    "                features = s[0]\n",
    "                text.append(s[1])\n",
    "        # append the last review\n",
    "        reviews.append(text)\n",
    "    return reviews\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
